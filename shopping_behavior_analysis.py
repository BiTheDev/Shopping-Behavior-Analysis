# -*- coding: utf-8 -*-
"""Shopping Behavior Analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vi_yEatxbRIuaJ8zvtP_0zYy7uUMKPuN
"""

from mpl_toolkits.mplot3d import Axes3D
import plotly.express as px
import seaborn as sns
from sklearn.impute import SimpleImputer
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
import numpy as np
from sklearn.linear_model import LinearRegression, LogisticRegression
from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, RandomForestRegressor, VotingClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import silhouette_score, accuracy_score, confusion_matrix, classification_report, mean_squared_error, mean_absolute_error, r2_score, explained_variance_score
import pandas as pd
from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder
from sklearn.svm import SVC
from scipy.stats import chi2_contingency
import scipy.stats as stats
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split, cross_val_score, KFold, GridSearchCV

# preprocessing the data
url = 'https://raw.githubusercontent.com/BiTheDev/Shopping-Behavior-Analysis/main/shopping_behavior_updated.csv'
data = pd.read_csv(url)

# Copying the dataset to avoid modifying the original data
preprocessed_data = data.copy()

# Encoding categorical variables using LabelEncoder
label_encoders = {}
print(preprocessed_data.head())
print(preprocessed_data["Category"].value_counts())

categorical_columns = ['Gender', 'Item Purchased', 'Category', 'Location', 'Size', 'Color',
                       'Season', 'Subscription Status', 'Shipping Type', 'Discount Applied',
                       'Promo Code Used', 'Payment Method', 'Frequency of Purchases']

for column in categorical_columns:
    label_encoders[column] = LabelEncoder()
    preprocessed_data[column] = label_encoders[column].fit_transform(
        preprocessed_data[column])

for column, encoder in label_encoders.items():
    print(f"Column: {column}")
    print(f"Original Values: {encoder.classes_}")
    print(f"Encoded Labels: {encoder.transform(encoder.classes_)}")
    print()
preprocessed_data.fillna(0, inplace=True)
# Normalizing numerical variables
numerical_columns = [
    'Review Rating', 'Previous Purchases']
scaler = StandardScaler()
preprocessed_data[numerical_columns] = scaler.fit_transform(
    preprocessed_data[numerical_columns])

# Check for NaN values in each column
nan_columns = preprocessed_data.columns[preprocessed_data.isna(
).any()].tolist()

# Check for infinite values in each column
inf_columns = preprocessed_data.columns[(
    preprocessed_data.abs() == np.inf).any()].tolist()

# Combine both lists to get columns with either NaN or infinite values
columns_with_issues = list(set(nan_columns + inf_columns))

# %%
preprocessed_data.head(5)
preprocessed_data.isna().sum()

# %%
df = preprocessed_data.copy()
df_segments = df[["Age", "Gender", "Season", "Subscription Status", "Category",
                  "Purchase Amount (USD)", "Frequency of Purchases"]]

wcss = []
for i in range(1, 8):
    # Create a KMeans instance with the current number of clusters
    kmeans = KMeans(n_clusters=i)
    kmeans.fit(df)  # Fit the model to the data
    wcss_iter = kmeans.inertia_  # Get the WCSS for the current number of clusters
    wcss.append(wcss_iter)  # Append the WCSS to the list

number_clusters = range(1, 8)
plt.plot(number_clusters, wcss)
plt.title('The Elbow Method')  # Set the title of the plot
plt.xlabel('Number of clusters')  # Set the label for the X-axis
plt.ylabel('WCSS')  # Set the label for the Y-axis

plt.show()

kmeans = KMeans(n_clusters=3, random_state=42)
kmeans.fit(df_segments)
cluster_labels = kmeans.predict(df_segments)
df_segments["Cluster"] = cluster_labels

df_segments
plt.scatter(
    df_segments[['Age']],  # X-axis: Age
    df_segments['Purchase Amount (USD)'],  # Y-axis: Purchase Amount (USD)
    c=kmeans.labels_,  # Color points by cluster labels assigned by KMeans
    cmap='viridis'  # Specify a color map for better visualization
)

plt.title('Customer Segments Using K-Means')
plt.xlabel('Age')
plt.ylabel('Purchase Amount (USD)')
plt.show()


fig = plt.figure(figsize=(10, 8))
ax = fig.add_subplot(111, projection='3d')
# print(df_segments.columns)
scatter = ax.scatter(
    df_segments['Age'],
    df_segments['Purchase Amount (USD)'],
    df_segments['Frequency of Purchases'],
    c=df_segments['Cluster'],
    cmap='viridis',
    marker='o',  # Set marker style
    s=50  # Set marker size
)

ax.set_xlabel('Age')
ax.set_ylabel('Purchase Amount (USD)')
ax.set_zlabel('Frequency of Purchases')
ax.set_title('Customer Segments Using K-Means (3D)')

colorbar = plt.colorbar(scatter)
colorbar.set_label('Cluster')

plt.show()


X = df[["Age", "Gender", "Season", "Subscription Status", "Category",
        "Frequency of Purchases", "Purchase Amount (USD)"]]
y = df_segments["Cluster"]  # Target variable

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42)

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

knn = KNeighborsClassifier(n_neighbors=5)

knn.fit(X_train_scaled, y_train)

y_pred = knn.predict(X_test_scaled)

accuracy = accuracy_score(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred)
print(f"Accuracy: {accuracy}")
print(classification_report(y_test, y_pred))

X = df[["Age", "Gender", "Season", "Subscription Status", "Category",
        "Frequency of Purchases", "Purchase Amount (USD)"]]
y = df_segments["Cluster"]  # Target variable

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42)

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

knn = KNeighborsClassifier()

param_grid = {'n_neighbors': np.arange(1, 31)}

grid_search = GridSearchCV(knn, param_grid, cv=5, scoring='accuracy')
grid_search.fit(X_train_scaled, y_train)

best_params = grid_search.best_params_
print(f"Best Hyperparameters: {best_params}")

best_knn = grid_search.best_estimator_
y_pred = best_knn.predict(X_test_scaled)
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy on Test Set: {accuracy}")


rf_classifier = RandomForestClassifier(
    n_estimators=100, random_state=42, class_weight='balanced')
svm_classifier = SVC(probability=True)
gb_classifier = GradientBoostingClassifier()


svm_classifier.fit(X_train, y_train)
gb_classifier.fit(X_train, y_train)
rf_classifier.fit(X_train, y_train)

# feature importance
feature_importances = rf_classifier.feature_importances_

feature_importance_df = pd.DataFrame(
    {'Feature': X.columns, 'Importance': feature_importances})

feature_importance_df = feature_importance_df.sort_values(
    by='Importance', ascending=False)


plt.figure(figsize=(10, 6))
plt.barh(feature_importance_df['Feature'], feature_importance_df['Importance'])
plt.xlabel('Importance')
plt.title('Feature Importances')
plt.show()


rf_accuracy = accuracy_score(y_test, rf_classifier.predict(X_test))
svm_accuracy = accuracy_score(y_test, svm_classifier.predict(X_test))
gb_accuracy = accuracy_score(y_test, gb_classifier.predict(X_test))
print(f"Random Forest Accuracy: {rf_accuracy:.2f}")
print(classification_report(y_test, rf_classifier.predict(X_test)))
print(f"SVM Accuracy: {svm_accuracy:.2f}")
print(classification_report(y_test, svm_classifier.predict(X_test)))
print(f"Gradient Boosting Accuracy: {gb_accuracy:.2f}")
print(classification_report(y_test, gb_classifier.predict(X_test)))

voting_classifier = VotingClassifier(estimators=[
    ('rf', rf_classifier),
    ('svm', svm_classifier),
    ('gb', gb_classifier)
], voting='soft')  # 'soft' allows for probability voting

voting_classifier.fit(X_train, y_train)

y_pred_voting = voting_classifier.predict(X_test)

accuracy_voting = accuracy_score(y_test, y_pred_voting)
print(f"Voting Classifier Accuracy: {accuracy_voting:.2f}")
print(classification_report(y_test, y_pred_voting))

sns.histplot(data=preprocessed_data, x="Age")
plt.title("Age Distribution")
plt.xlabel("Age")
plt.ylabel("Count")
plt.show()
#%%
sns.histplot(data=preprocessed_data, x="Age", hue="Gender", multiple="dodge", shrink=.8)
plt.title("Age Distribution by Gender")
plt.xlabel("Age")
plt.ylabel("Count")
plt.show()
#%%
sns.histplot(data=preprocessed_data, x="Category", shrink=.8)
plt.title("Purchased Item Category Distribution")
plt.xlabel("Category")
plt.ylabel("Count")
plt.show()
#%%
sns.histplot(data=preprocessed_data, x="Season", shrink=.8)
plt.title("Shopping Season Distribution")
plt.xlabel("Season")
plt.ylabel("Count of purchase")
plt.show()
#%%
sns.histplot(data=preprocessed_data, x="Frequency of Purchases", shrink=.8)
plt.title("Frequency of Purchases Distribution")
plt.xlabel("Frequency")
plt.xticks(rotation = 45)
plt.ylabel("Count")
plt.show()
#%%
arrFall=preprocessed_data['Season'] == 'Fall'
arrSpring=preprocessed_data['Season'] == 'Spring'
arrSummer=preprocessed_data['Season'] == 'Summer'
arrWinter=preprocessed_data['Season'] == 'Winter'

vFall = preprocessed_data[arrFall]
vSpring = preprocessed_data[arrSpring]
vSummer = preprocessed_data[arrSummer]
vWinter= preprocessed_data[arrWinter]

fallSales = vFall['Purchase Amount (USD)'].sum()
springSales = vSpring['Purchase Amount (USD)'].sum()
summerSales = vSummer['Purchase Amount (USD)'].sum()
winterSales = vWinter['Purchase Amount (USD)'].sum()

fallDiscounts = vFall['Discount Applied'].value_counts()['Yes']
springDiscounts = vSpring['Discount Applied'].value_counts()['Yes']
summerDiscounts = vSummer['Discount Applied'].value_counts()['Yes']
winterDiscounts = vWinter['Discount Applied'].value_counts()['Yes']

fallDiscountsN = vFall['Discount Applied'].value_counts()['No']
springDiscountsN = vSpring['Discount Applied'].value_counts()['No']
summerDiscountsN = vSummer['Discount Applied'].value_counts()['No']
winterDiscountsN = vWinter['Discount Applied'].value_counts()['No']

fallSalesArr = vFall[['Purchase Amount (USD)', 'Discount Applied']]
springSalesArr = vSpring[['Purchase Amount (USD)', 'Discount Applied']]
summerSalesArr = vSummer[['Purchase Amount (USD)', 'Discount Applied']]
winterSalesArr = vWinter[['Purchase Amount (USD)', 'Discount Applied']]

fallSalesDis = fallSalesArr['Discount Applied'] == 'Yes'
springSalesDis = springSalesArr['Discount Applied'] == 'Yes'
summerSalesDis = summerSalesArr['Discount Applied'] == 'Yes'
winterSalesDis = winterSalesArr['Discount Applied'] == 'Yes'

stats.ttest_ind(a=springSalesDis, b=summerSalesDis, equal_var=True)

stats.ttest_ind(a=springSalesDis, b=fallSalesDis, equal_var=True)

stats.ttest_ind(a=springSalesDis, b=winterSalesDis, equal_var=True)

stats.ttest_ind(a=summerSalesDis, b=fallSalesDis, equal_var=True)

stats.ttest_ind(a=summerSalesDis, b=winterSalesDis, equal_var=True)

stats.ttest_ind(a=fallSalesDis, b=winterSalesDis, equal_var=True)

data = [['Winter', winterSales, winterDiscounts], ['Spring', springSales, springDiscounts],
        ['Summer', summerSales, summerDiscounts], ['Fall', fallSales, fallDiscounts]]
sales = pd.DataFrame(data, columns=['Season', 'Sales', 'Discount count'])

chi_data = [[winterDiscounts.astype('int'), springDiscounts.astype('int'), summerDiscounts.astype('int'), fallDiscounts.astype('int')],
            [winterDiscountsN.astype('int'), springDiscountsN.astype('int'), summerDiscountsN.astype('int'), fallDiscountsN.astype('int')]]
stat, p, dof, expected = chi2_contingency(chi_data)
chi_data = pd.DataFrame(chi_data)
chi_data.columns= ['Winter', 'Spring', 'Summer','Fall']
chi_data.index = ['Discount used', 'Discount not used']
print(pd.DataFrame(chi_data))
significance_level = 0.05
print("p value: " + str(p))
if p <= significance_level:
    print('Reject NULL HYPOTHESIS')
else:
    print('ACCEPT NULL HYPOTHESIS')
sales.plot(x='Season', y='Sales', kind='bar', legend=False)
plt.xlabel('Season')
plt.xticks(rotation = 0)
plt.ylabel('Sales in $')
plt.title('Best Season Histogram')
plt.show()

sales.plot(x='Season', y='Discount count', kind='bar', legend=False)
plt.xlabel('Season')
plt.xticks(rotation = 0)
plt.ylabel('Number of discount used')
plt.title('Discount Histogram')
plt.show()

# One-hot encode categorical features including 'Season'
categorical_features = ['Gender', 'Location', 'Subscription Status', 'Season', 'Shipping Type', 'Discount Applied', 'Promo Code Used', 'Payment Method', 'Frequency of Purchases']
numerical_features = ['Age', 'Previous Purchases', 'Review Rating']

preprocessor = ColumnTransformer(transformers=[
    ('cat', OneHotEncoder(), categorical_features),
    ('num', 'passthrough', numerical_features)
])

# Assuming that we are predicting a purchase within the 'Clothing' category
y = (data['Category'] == 'Clothing').astype(int)
X = data.drop(columns=['Customer ID', 'Item Purchased', 'Category', 'Purchase Amount (USD)', 'Size', 'Color'])

# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create and train the RandomForestClassifier within a Pipeline
pipeline = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('classifier', RandomForestClassifier(random_state=42))
])
pipeline.fit(X_train, y_train)

# Predict and evaluate the model
y_pred = pipeline.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
report = classification_report(y_test, y_pred)

# Display results
print(f"Accuracy for predicting clothing purchases: {accuracy}")
print("Classification report: \n", report)

# Feature importance
feature_importances = pipeline.named_steps['classifier'].feature_importances_
feature_names = preprocessor.transformers_[0][1].get_feature_names_out(categorical_features)
feature_names = np.concatenate([feature_names, numerical_features])

# Combine feature importances with feature names
feature_importance_dict = dict(zip(feature_names, feature_importances))

# Sort the feature importances in descending order and print
sorted_feature_importances = sorted(feature_importance_dict.items(), key=lambda item: item[1], reverse=True)
print("Feature importances: ", sorted_feature_importances)


# Define the SVC pipeline
pipeline_svc = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('classifier', SVC(random_state=42))
])

# Parameter grid for GridSearchCV
param_grid = {
    'classifier__C': [0.1, 1, 10],
    'classifier__kernel': ['linear', 'rbf'],
    'classifier__gamma': ['scale', 'auto']
}

# Setup GridSearchCV
grid_search = GridSearchCV(pipeline_svc, param_grid, cv=5, scoring='accuracy')

# Fit the model
grid_search.fit(X_train, y_train)

# Get the best estimator
best_svc = grid_search.best_estimator_

# Predictions
y_pred_svc = best_svc.predict(X_test)

# Evaluation
accuracy_svc = accuracy_score(y_test, y_pred_svc)
report_svc = classification_report(y_test, y_pred_svc)

# Display results
print(f"Best parameters: {grid_search.best_params_}")
print(f"Best SVC accuracy: {accuracy_svc}")
print("SVC Classification report: \n", report_svc)


#%% hyperparameter tuning

# One-hot encode categorical features including 'Season'
categorical_features = ['Gender', 'Location', 'Subscription Status', 'Season', 'Shipping Type', 'Discount Applied', 'Promo Code Used', 'Payment Method', 'Frequency of Purchases']
numerical_features = ['Age', 'Previous Purchases', 'Review Rating']

preprocessor = ColumnTransformer(transformers=[
    ('cat', OneHotEncoder(), categorical_features),
    ('num', 'passthrough', numerical_features)
])

# Assuming that we are predicting a purchase within the 'Clothing' category
y = (data['Category'] == 'Clothing').astype(int)
X = data.drop(columns=['Customer ID', 'Item Purchased', 'Category', 'Purchase Amount (USD)', 'Size', 'Color'])

# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# SVC pipeline
pipeline = Pipeline([
    ('preprocessor', preprocessor),
    ('svc', SVC(random_state=42))
])

# Parameter grid for GridSearch
param_grid = {
    'svc__C': [0.1, 1, 10],
    'svc__gamma': [1, 0.1, 0.01],
    'svc__kernel': ['rbf', 'linear']
}

# Grid search with cross-validation
grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy')
grid_search.fit(X_train, y_train)

# Best parameters found
print("Best parameters found: ", grid_search.best_params_)

# Best model
best_model = grid_search.best_estimator_

# Predict and evaluate the model
y_pred = best_model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
report = classification_report(y_test, y_pred)

# Display results
print(f"Accuracy for predicting clothing purchases with tuned SVC: {accuracy}")
print("Classification report with tuned SVC: \n", report)


#%% Number of Clothing Purchases by Age Group

# Filter data for 'Clothing' category and group by age
clothing_data = data[data['Category'] == 'Clothing']
age_group_counts = clothing_data.groupby('Age').size()

# Plot the distribution of clothing purchases by age
plt.figure(figsize=(12, 6))
sns.barplot(x=age_group_counts.index, y=age_group_counts.values)
plt.title('Number of Clothing Purchases by Age Group')
plt.xlabel('Age')
plt.ylabel('Number of Purchases')
plt.xticks(rotation=45)
plt.show()



#%% Gender Distribution in Clothing Purchases

clothing_data = data[data['Category'] == 'Clothing']
gender_counts = clothing_data['Gender'].value_counts()

plt.figure(figsize=(8, 5))
sns.barplot(x=gender_counts.index, y=gender_counts.values)
plt.title('Gender Distribution in Clothing Purchases')
plt.xlabel('Gender')
plt.ylabel('Number of Purchases')
plt.show()


#%% Purchase Amount Distribution in Clothing Category

plt.figure(figsize=(8, 5))
sns.histplot(clothing_data['Purchase Amount (USD)'], bins=20, kde=True)
plt.title('Purchase Amount Distribution in Clothing Category')
plt.xlabel('Purchase Amount (USD)')
plt.ylabel('Frequency')
plt.show()


#%% Purchase Frequency by Location for Clothing Category
location_counts = clothing_data['Location'].value_counts()
plt.figure(figsize=(12, 6))
sns.barplot(x=location_counts.index, y=location_counts.values)
plt.title('Purchase Frequency by Location for Clothing Category')
plt.xlabel('Location')
plt.ylabel('Number of Purchases')
plt.xticks(rotation=90)
plt.show()

#%%
# Question: Predict the review rating a customer is likely to give

# Plot the review ratings to visualize the distribution
plt.hist(preprocessed_data["Review Rating"], bins= 20, edgecolor='black', alpha=0.7)
plt.xticks(np.arange(1.0, 6.0))
plt.title("Distribution Of The Review Rating")
plt.xlabel("Review Ratings")
plt.ylabel("Frequency")
plt.grid(True)
plt.show()

# Encoding categorical variables using LabelEncoder
label_encoders = {}
categorical_columns = ['Gender', 'Item Purchased', 'Category', 'Location', 'Size', 'Color',
                       'Season', 'Subscription Status', 'Shipping Type', 'Discount Applied',
                       'Promo Code Used', 'Payment Method', 'Frequency of Purchases']

for column in categorical_columns:
    label_encoders[column] = LabelEncoder()
    preprocessed_data[column] = label_encoders[column].fit_transform(preprocessed_data[column])

# Normalizing numerical variables
numerical_columns = ['Age', 'Purchase Amount (USD)', 'Previous Purchases', "Review Rating"]
scaler = StandardScaler()
preprocessed_data[numerical_columns] = scaler.fit_transform(preprocessed_data[numerical_columns])

# Displaying the first few rows of the preprocessed dataset
print(preprocessed_data.head())


# Understand feature correction
correlation_matrix = preprocessed_data.corr()

# Plot a heatmap to visualize the correlation matrix
plt.figure(figsize=(15, 10))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', linewidths=.5)
plt.title('Feature Correlation Matrix')
plt.show()


# Define the feature columns and target column
feature_cols = ["Age", "Gender", "Item Purchased",
                "Purchase Amount (USD)", "Location",
                "Previous Purchases", "Frequency of Purchases",
                'Discount Applied', 'Promo Code Used']
target_col = "Review Rating"

X = preprocessed_data[feature_cols]
y = preprocessed_data[target_col]

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=10)

#%%
# Create a linear regression model
lr_model = LinearRegression()

# Define hyperparameters to tune
lr_param_grid = {'fit_intercept': [True, False]}

# Use k-fold cross-validation for hyperparameter tuning
lr_kf = KFold(n_splits=9, shuffle=True, random_state=40)

# GridSearchCV for hyperparameter tuning
lr_grid_search = GridSearchCV(lr_model, lr_param_grid, scoring='neg_mean_squared_error', cv=lr_kf)
lr_grid_search.fit(X_train, y_train)

# Display the best hyperparameters and best model
print("Best Hyperparameters:", lr_grid_search.best_params_)
best_lr_model = lr_grid_search.best_estimator_

# Evaluate the best model on the test set
lr_y_pred = best_lr_model.predict(X_test)

lr_mse = mean_squared_error(y_test, lr_y_pred)
lr_mae = mean_absolute_error(y_test, lr_y_pred)
lr_r2 = r2_score(y_test, lr_y_pred)
lr_explained_variance = explained_variance_score(y_test,lr_y_pred)

# Display the score results
print(f"{lr_model.__class__.__name__}: ")
print(f"Mean Squared Error is {lr_mse:.2f}")
print(f"Mean Absolute Error is {lr_mae:.2f} ")
print(f"R-squared is {lr_r2:.2f}")
print(f"Explained Variance Score is {lr_explained_variance: .2f} \n")

# Get coefficients of the linear regression model as
# an indicator of feature importance.
coefficients = lr_model.fit(X, y).coef_

# Print feature coefficients
print("Feature coefficients:")
for feature, coefficient in zip(X_train.columns, coefficients):
    print(f"{feature}: {coefficient}")

print("\n")

# Define the RandomForestRegressor
rf_model = RandomForestRegressor()

# Define the parameter grid to search
rf_param_grid = {
    'n_estimators': [100, 200, 300],
    'max_features': ['sqrt', 'log2'],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

# Create a KFold cross-validation object
rf_kf = KFold(n_splits=3, shuffle=True, random_state=15)

# Create the GridSearchCV object
rf_grid_search = GridSearchCV(estimator=rf_model, param_grid=rf_param_grid, scoring='neg_mean_squared_error', cv=rf_kf)

# Fit the model with the data
rf_grid_search.fit(X, y)

# Get the best parameters and best model
rf_best_params = rf_grid_search.best_params_
rf_best_model = rf_grid_search.best_estimator_

# Print the best parameters
print(f"Best Parameters: {rf_best_params}")

# Use the best model to make predictions
rf_y_pred = rf_best_model.predict(X_test)

# Evaluate the model
rf_mse = mean_squared_error(y_test, rf_y_pred)
rf_mae = mean_absolute_error(y_test, rf_y_pred)
rf_r2 = r2_score(y_test, rf_y_pred)
rf_explained_variance = explained_variance_score(y_test, rf_y_pred)

# Display the score results
print(f"{rf_model.__class__.__name__}:")
print(f"Mean Squared Error is {rf_mse:.2f}")
print(f"Mean Absolute Error is {rf_mae:.2f} ")
print(f"R-squared is {rf_r2:.2f}")
print(f"Explained Variance Score is {rf_explained_variance:.2f} \n")

# Get feature importances
importances = rf_model.fit(X, y).feature_importances_
# Sort indices in descending order of importance
indices = np.argsort(importances)[::-1]
# Print feature ranking
print("Feature ranking:")
for f in range(X_train.shape[1]):
    print(f"{X_train.columns[indices[f]]}: {importances[indices[f]]}")
print("------------------------------------")

# Visualize the feature importance
plt.figure(figsize=(10, 6))
plt.bar(range(X_train.shape[1]), importances[indices], align="center")
plt.xticks(range(X_train.shape[1]), X_train.columns[indices], rotation=90)
plt.xlabel("Feature")
plt.ylabel("Importance Score")
plt.title("Feature Importance for Random Forest Regressor")
plt.show()

#%% Visualize the comparison scores of two models
# Create lists for each metric and model
metrics = ['Mean Squared Error (MSE)', 'Mean Absolute Error (MAE)', 'R-squared', 'Explained Variance']
lr_scores = [lr_mse, lr_mae, lr_r2, lr_explained_variance]
rf_scores = [rf_mse, rf_mae, rf_r2, rf_explained_variance]

# Plotting
fig, ax = plt.subplots(figsize=(10, 6))

bar_width = 0.35
index = range(len(metrics))

bar1 = ax.bar(index, lr_scores, bar_width, label='Linear Regression')
bar2 = ax.bar([i + bar_width for i in index], rf_scores, bar_width, label='Random Forest Regressor')

# Add labels, title, and legend
ax.set_xlabel('Metrics')
ax.set_ylabel('Scores')
ax.set_title('Comparison of Model Scores')
ax.set_xticks([i + bar_width / 2 for i in index])
ax.set_xticklabels(metrics)
ax.legend()

# Display the plot
plt.show()